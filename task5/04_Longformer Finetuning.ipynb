{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe748b15-d964-4995-9b6e-edee1dc10a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def bootstrap_ci(y_test, y_pred, calculate_metric, confidence=0.9, n_resamples=1000, sample_fraction=0.95, random_state=None):\n",
    "    \"\"\"\n",
    "    Compute bootstrap confidence interval for a metric function.\n",
    "    Parameters:\n",
    "    - y_test: array-like, true labels.\n",
    "    - y_pred: array-like, predicted labels.\n",
    "    - calculate_metric: function, computes the metric given y_test and y_pred.\n",
    "    - confidence: float, confidence level (default 0.9).\n",
    "    - n_resamples: int, number of bootstrap resamples (default 100).\n",
    "    - sample_fraction: float, fraction of data to sample in each resample (default 0.95).\n",
    "    - random_state: int or None, random seed for reproducibility.\n",
    "    Returns:\n",
    "    - (lower_bound, upper_bound): tuple of floats, confidence interval bounds.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n = len(y_test)\n",
    "    sample_size = int(n * sample_fraction)\n",
    "    metrics = []\n",
    "    for _ in range(n_resamples):\n",
    "        indices = rng.choice(n, size=sample_size, replace=True)\n",
    "        metric = calculate_metric(np.array(y_test)[indices], np.array(y_pred)[indices])\n",
    "        metrics.append(metric)\n",
    "    alpha = (1 - confidence) / 2\n",
    "    lower = np.percentile(metrics, 100 * alpha)\n",
    "    upper = np.percentile(metrics, 100 * (1 - alpha))\n",
    "    return {\"metric\": round(calculate_metric(y_test, y_pred), 4), \"confidence_interval\": (round(lower, 4), round(upper, 4))}\n",
    "\n",
    "# def calculate_metric(y_true, y_pred):\n",
    "#     # Example: Mean Absolute Error\n",
    "#     return np.mean(np.abs(np.array(y_true) - np.array(y_pred)))\n",
    "# y_test = [1, 2, 3, 4, 5]\n",
    "# y_pred = [1.1, 1.9, 3.2, 3.8, 5.1]\n",
    "\n",
    "# print(bootstrap_ci(y_test, y_pred, calculate_metric, confidence=0.9))\n",
    "\n",
    "# def accuracy_metric(y_true, y_pred):\n",
    "#     return np.mean(y_true == y_pred)\n",
    "# bootstrap_ci(true_labels, preds, accuracy_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c79c48-03fb-4cbb-8a0e-3419d5b51055",
   "metadata": {},
   "source": [
    "## Retrain with some edits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4bebad1-a82a-44ee-8f86-de2f400d6f46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b998ffbcfe9440e6bd25d6159039ad80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1652 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9b9489fb3344c5b185c01a3ed28204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/415 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_65358/4142805982.py:85: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`label` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/acl-env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:777\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 777\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m~/acl-env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:739\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     95\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 96\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     99\u001b[0m predictions \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(tokenized_test_dataset)\n",
      "File \u001b[0;32m~/acl-env/lib/python3.10/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/acl-env/lib/python3.10/site-packages/transformers/trainer.py:2480\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2478\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2479\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2480\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2481\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[1;32m   2482\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/acl-env/lib/python3.10/site-packages/transformers/trainer.py:5153\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[1;32m   5151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5152\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5153\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m   5154\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5155\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/acl-env/lib/python3.10/site-packages/accelerate/data_loader.py:566\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 566\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/acl-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/acl-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/acl-env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/acl-env/lib/python3.10/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/acl-env/lib/python3.10/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[0;32m~/acl-env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3388\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3385\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3386\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/acl-env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:241\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    237\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/acl-env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:793\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    789\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    790\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    791\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    792\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    794\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    797\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    798\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`label` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load your dataset and drop rows with missing responses\n",
    "df = pd.read_csv(\"./mrbench_v3_devset_train_data.csv\").dropna(subset=[\"response\"])\n",
    "\n",
    "# Convert string labels to integer indices\n",
    "# Create mapping dictionaries for encoding and later decoding\n",
    "label_list = sorted(df[\"label\"].unique())\n",
    "label_to_id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "# Replace string labels with integer labels in the DataFrame\n",
    "# Split by conversation_id\n",
    "conversation_ids = df[\"conversation_id\"].unique()\n",
    "train_ids, test_ids = train_test_split(conversation_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "train_df = df[df[\"conversation_id\"].isin(train_ids)].reset_index(drop=True)\n",
    "test_df = df[df[\"conversation_id\"].isin(test_ids)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_texts = [f\"{c} </s> <s> {r} </s>\" for c, r in zip(train_df[\"conversation_history\"].tolist(), train_df[\"response\"].tolist())]\n",
    "test_texts = [f\"{c} </s> <s> {r} </s>\" for c, r in zip(train_df[\"conversation_history\"].tolist(), test_df[\"response\"].tolist())]\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_df[\"label\"].tolist()})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_texts, \"label\": test_df[\"label\"].tolist()})\n",
    "\n",
    "# Initialize tokenizer & model\n",
    "model_name = \"allenai/longformer-base-4096\"\n",
    "tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
    "model = LongformerForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=len(label_list),\n",
    "    # attention_probs_dropout_prob=0.2,\n",
    "    # hidden_dropout_prob=0.2\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024\n",
    "    )\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Compute metrics function for evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",  # evaluates at the end of each epoch\n",
    "    save_strategy=\"epoch\",          # saves checkpoint at each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    # dropout=0.3\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = trainer.predict(tokenized_test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Generate detailed classification report (using integer labels)\n",
    "print(\"Detailed Classification Report (Integer Labels):\")\n",
    "print(classification_report(true_labels, preds, digits=4))\n",
    "\n",
    "# Generate classification report with target names (map back to original string labels)\n",
    "target_names = [id_to_label[i] for i in range(len(label_list))]\n",
    "print(\"Detailed Classification Report (String Labels):\")\n",
    "print(classification_report(true_labels, preds, target_names=target_names, digits=4))\n",
    "\n",
    "# --- Save the Fine-tuned Model ---\n",
    "# Before saving, update the model configuration with label mappings so that id2label and label2id\n",
    "# are stored in the saved config.json. This is necessary because categorical labels are not handled by default.\n",
    "model.config.id2label = id_to_label\n",
    "model.config.label2id = label_to_id\n",
    "\n",
    "# Save the model and tokenizer; the config will be saved automatically\n",
    "save_directory = \"./saved_model\"\n",
    "trainer.save_model(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15dc6023-6cb3-431b-901d-5fbcccc5ef96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Tutor: Hi, could you please provide a step-by-step solution for the question below? The question is: Tyson decided to make muffaletta sandwiches for the big game.  Each sandwich required 1 pound each of meat and cheese and would serve 4 people.  There would be 20 people in total watching the game.  The meat cost $7.00 per pound and the cheese cost $3.00 per pound.  How much money would he spend on the meat and cheese to make enough sandwiches to serve 20 people? \\n Student: To serve 20 people, Tyson needs to make 20/4 = 5 sandwiches.\\nEach sandwich requires 1+1 = 2 pounds of meat and cheese.\\nFor 5 sandwiches, he needs a total of 2 x 5 = 10 pounds of meat and cheese.\\nThe cost of 10 pounds of meat is 10 x $7.00 = $70.\\nThe cost of 10 pounds of cheese is 10 x $3.00 = $30.\\nThe total cost of meat and cheese is $70 + $30 = $100.\\n 100 \\n Tutor: do you want to talk me through your solution \\n Student: Yes I think my answer is correct. I used 10 pounds of meat and 10 pounds of cheese so I multiplied 7 dollars times 10 pounds for the meat and 3 dollars times 10 pounds for the cheese. That gave me a total of $100 for the meat and cheese </s> <s> It looks like you've done a great job figuring out how many pounds of meat and cheese Tyson needs, but remember that he uses 1 pound of each for every sandwich! </s>\", \"Tutor: Hi, could you please provide a step-by-step solution for the question below? The question is: Tyson decided to make muffaletta sandwiches for the big game.  Each sandwich required 1 pound each of meat and cheese and would serve 4 people.  There would be 20 people in total watching the game.  The meat cost $7.00 per pound and the cheese cost $3.00 per pound.  How much money would he spend on the meat and cheese to make enough sandwiches to serve 20 people? \\n Student: To serve 20 people, Tyson needs to make 20/4 = 5 sandwiches.\\nEach sandwich requires 1+1 = 2 pounds of meat and cheese.\\nFor 5 sandwiches, he needs a total of 2 x 5 = 10 pounds of meat and cheese.\\nThe cost of 10 pounds of meat is 10 x $7.00 = $70.\\nThe cost of 10 pounds of cheese is 10 x $3.00 = $30.\\nThe total cost of meat and cheese is $70 + $30 = $100.\\n 100 \\n Tutor: do you want to talk me through your solution \\n Student: Yes I think my answer is correct. I used 10 pounds of meat and 10 pounds of cheese so I multiplied 7 dollars times 10 pounds for the meat and 3 dollars times 10 pounds for the cheese. That gave me a total of $100 for the meat and cheese </s> <s> You've done a great job, but there's a small misunderstanding. You're right that you need 10 pounds total, but remember that it's 5 pounds of meat and 5 pounds of cheese, not 10 pounds of each. Let's recalculate using this. </s>\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1547/1547 [00:04<00:00, 347.03 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_11829/2645995393.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  json.dump(df.groupby([\"conversation_id\", \"conversation_history\"], sort=False).apply(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"./mrbench_v3_testset.json\", \"r\") as f:\n",
    "    test_set_json = json.load(f)\n",
    "\n",
    "data = []\n",
    "for entry in test_set_json:\n",
    "    for key in entry[\"tutor_responses\"]:\n",
    "        data.append([entry[\"conversation_id\"], entry[\"conversation_history\"], key, entry[\"tutor_responses\"][key][\"response\"]])\n",
    "df = pd.DataFrame(data, columns=[\"conversation_id\", \"conversation_history\", \"tutor_id\", \"tutor_response\"])\n",
    "\n",
    "with open(\"./saved_model/config.json\", \"r\") as f:\n",
    "    id2label = json.load(f)[\"id2label\"]\n",
    "\n",
    "# Now make Hugging Face Dataset objects manually\n",
    "test_texts = [f\"{c} </s> <s> {r} </s>\" for c, r in zip(df[\"conversation_history\"].tolist(), df[\"tutor_response\"].tolist())]\n",
    "test_dataset = Dataset.from_dict({\"text\": test_texts, \"label\": [0] * len(test_texts)})\n",
    "print(test_texts[:2])\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024\n",
    "    )\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Run predictions\n",
    "predictions = trainer.predict(tokenized_test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Map prediction indices to labels\n",
    "pred_labels = [id2label[str(label)] for label in preds]\n",
    "\n",
    "# Add predictions to the DataFrame for inspection\n",
    "df[\"predicted_label\"] = pred_labels\n",
    "\n",
    "# df.groupby([\"conversation_id\", \"conversation_history\"]).apply(lambda x: )\n",
    "\n",
    "with open(\"./saved_model/predictions.json\", \"w\") as f:\n",
    "    json.dump(df.groupby([\"conversation_id\", \"conversation_history\"], sort=False).apply(\n",
    "        lambda group: {\n",
    "            \"conversation_id\": group.name[0],\n",
    "            \"conversation_history\": group.name[1],\n",
    "            \"tutor_responses\": {\n",
    "                f\"Tutor_{i+1}\": {\n",
    "                    \"response\": row[\"tutor_response\"],\n",
    "                    \"annotation\": {\"Tutor_Identification\": row[\"predicted_label\"]}\n",
    "                }\n",
    "                for i, row in enumerate(group.to_dict(\"records\"))\n",
    "            }\n",
    "        }\n",
    "    ).tolist(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5f7ef1-3708-4978-8349-8fcf9aadff41",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Inference on the unlabeled test set and submission (first run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d82c6d1-a2d1-42e3-88d7-fb2d03c0a66d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "with open(\"./ACL/mrbench_v3_testset.json\", \"r\") as f:\n",
    "    test_set_json = json.load(f)\n",
    "\n",
    "data = []\n",
    "for entry in test_set_json:\n",
    "    for key in entry[\"tutor_responses\"]:\n",
    "        data.append([entry[\"conversation_id\"], entry[\"conversation_history\"], key, entry[\"tutor_responses\"][key]])\n",
    "df = pd.DataFrame(data, columns=[\"conversation_id\", \"conversation_history\", \"tutor_id\", \"tutor_response\"])\n",
    "\n",
    "    # {\n",
    "    #     \"conversation_id\": \"221-362eb11a-f190-42a6-b2a4-985fafdcfa9e\",\n",
    "    #     \"conversation_history\": \"Tutor: Hi, could you please provide a step-by-step solution for the question below? The question is: Tyson decided to make muffaletta sandwiches for the big game.  Each sandwich required 1 pound each of meat and cheese and would serve 4 people.  There would be 20 people in total watching the game.  The meat cost $7.00 per pound and the cheese cost $3.00 per pound.  How much money would he spend on the meat and cheese to make enough sandwiches to serve 20 people? \\n Student: To serve 20 people, Tyson needs to make 20/4 = 5 sandwiches.\\nEach sandwich requires 1+1 = 2 pounds of meat and cheese.\\nFor 5 sandwiches, he needs a total of 2 x 5 = 10 pounds of meat and cheese.\\nThe cost of 10 pounds of meat is 10 x $7.00 = $70.\\nThe cost of 10 pounds of cheese is 10 x $3.00 = $30.\\nThe total cost of meat and cheese is $70 + $30 = $100.\\n 100 \\n Tutor: How many pounds of meat are needed for each sandwich? \\n Student: Each sandwich requires 1 pound of meat and 1 pound of cheese. \\n Tutor: What is the cost of 1 pound of meat? \\n Student: The cost of 1 pound of meat is $7.00.\",\n",
    "    #     \"tutor_responses\": {\n",
    "    #         \"Sonnet\": {\n",
    "    #             \"response\": \"Great, you've correctly identified the cost of the meat, now let's focus on calculating the total cost of meat for all the sandwiches needed.\",\n",
    "    #             \"annotation\": {\n",
    "    #                 \"Mistake_Identification\": \"Yes\",\n",
    "    #                 \"Mistake_Location\": \"Yes\",\n",
    "    #                 \"Providing_Guidance\": \"Yes\",\n",
    "    #                 \"Actionability\": \"Yes\"\n",
    "    #             }\n",
    "    #         },\n",
    "    #         \"Llama318B\": {\n",
    "    #             \"response\": \"Now that we know the cost of 1 pound of meat is $7.00, we can use this information to find the correct total cost for the meat and cheese needed for the muffaletta sandwiches.\",\n",
    "    #             \"annotation\": {\n",
    "    #                 \"Mistake_Identification\": \"Yes\",\n",
    "    #                 \"Mistake_Location\": \"To some extent\",\n",
    "    #                 \"Providing_Guidance\": \"To some extent\",\n",
    "    #                 \"Actionability\": \"To some extent\"\n",
    "    #             }\n",
    "    #         },\n",
    "    #         \"Llama31405B\": {\n",
    "    #             \"response\": \"You're close, but I notice that you calculated the cost of 10 pounds of meat, when actually 5 pounds of meat are needed for 5 sandwiches, since each sandwich requires 1 pound of meat; can you recalculate the cost of the meat?\",\n",
    "    #             \"annota\n",
    "\n",
    "\n",
    "# Convert string labels to integer indices\n",
    "# Create mapping dictionaries for encoding and later decoding\n",
    "\n",
    "model_name = \"./ACL/saved_model\"\n",
    "tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
    "model = LongformerForSequenceClassification.from_pretrained(model_name, num_labels=9)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "with open(\"./ACL/saved_model/config.json\", \"r\") as f:\n",
    "    id2label = json.load(f)[\"id2label\"]\n",
    "\n",
    "# Now make Hugging Face Dataset objects manually\n",
    "test_texts = [f\"Conversation: {c}\\n\\nResponse: {r}\" for c, r in zip(df[\"conversation_history\"].tolist(), df[\"tutor_response\"].tolist())]\n",
    "test_dataset = Dataset.from_dict({\"text\": test_texts, \"label\": [0] * len(test_texts)})\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=4096\n",
    "    )\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(model=model, args=TrainingArguments(output_dir=\"./results_eval\", per_device_eval_batch_size=2, report_to=\"none\"))\n",
    "\n",
    "# Run predictions\n",
    "predictions = trainer.predict(tokenized_test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Map prediction indices to labels\n",
    "pred_labels = [id2label[str(label)] for label in preds]\n",
    "\n",
    "# Add predictions to the DataFrame for inspection\n",
    "df[\"predicted_label\"] = pred_labels\n",
    "\n",
    "# df.groupby([\"conversation_id\", \"conversation_history\"]).apply(lambda x: )\n",
    "\n",
    "with open(\"./ACL/predictions.json\", \"w\") as f:\n",
    "    json.dump(df.groupby([\"conversation_id\", \"conversation_history\"], sort=False).apply(\n",
    "        lambda group: {\n",
    "            \"conversation_id\": group.name[0],\n",
    "            \"conversation_history\": group.name[1],\n",
    "            \"tutor_responses\": {\n",
    "                f\"Tutor_{i+1}\": {\n",
    "                    \"response\": row[\"tutor_response\"][\"response\"],\n",
    "                    \"annotation\": {\"Tutor_Identification\": row[\"predicted_label\"]}\n",
    "                }\n",
    "                for i, row in enumerate(group.to_dict(\"records\"))\n",
    "            }\n",
    "        }\n",
    "    ).tolist(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3481793d-5f14-40ea-9d57-589d1bced3b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1957b60-834f-4e5c-a0e3-167289dbdd80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "acl-env",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python (acl-env) (Local)",
   "language": "python",
   "name": "acl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
