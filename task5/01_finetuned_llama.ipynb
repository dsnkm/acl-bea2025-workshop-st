{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea34cbc6-4a00-4a7c-a5ee-e2f0e2f0aa11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fae20bb-9725-4dcb-bde0-2468663da5f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------\n",
    "# Load Dataset and Perform Grouped Split\n",
    "# --------------------------------------\n",
    "\n",
    "# Convert to pandas for grouping\n",
    "df = pd.read_csv(\"./data/mrbench_v3_devset_train_data.csv\").dropna(subset=[\"response\"])\n",
    "df[\"text\"] = \"Conversation:\\n\" + df[\"conversation_history\"] + \"\\n\\nResponse:\\n\" + df[\"response\"] + \"\\n\\n\\nPrediction: \"\n",
    "\n",
    "# Split by conversation_id\n",
    "conversation_ids = df[\"conversation_id\"].unique()\n",
    "train_ids, test_ids = train_test_split(conversation_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = df[df[\"conversation_id\"].isin(train_ids)].reset_index(drop=True)\n",
    "test_df = df[df[\"conversation_id\"].isin(test_ids)].reset_index(drop=True)\n",
    "print(train_df.head())\n",
    "\n",
    "# Convert back to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Optional: Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# -----------------------\n",
    "# Tokenizer and Model Setup\n",
    "# -----------------------\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Quantization config\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# -----------------------\n",
    "# PEFT Configuration (LoRA)\n",
    "# -----------------------\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.5,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# -----------------------\n",
    "# Fine-Tuning\n",
    "# -----------------------\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    report_to=\"none\",\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# -----------------------\n",
    "# Save the Fine-Tuned Model\n",
    "# -----------------------\n",
    "peft_model.save_pretrained(\"./fine-tuned-llama2\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ce4af-b8cf-4f77-8aa8-ab8d309cf53c",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a177842-97da-4456-8936-bb0b5767efb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]=\"expandable_segments:True\"\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    correct = sum(p.strip() == l.strip() for p, l in zip(decoded_preds, decoded_labels))\n",
    "    accuracy = correct / len(decoded_labels)\n",
    "    \n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "# Convert to pandas for grouping\n",
    "df = pd.read_csv(\"./data/mrbench_v3_devset_train_data.csv\").dropna(subset=[\"response\"])\n",
    "df[\"text\"] = \"Conversation:\\n\" + df[\"conversation_history\"] + \"\\n\\nResponse:\\n\" + df[\"response\"] + \"\\n\\n\\nPrediction: \"\n",
    "\n",
    "# Split by conversation_id\n",
    "conversation_ids = df[\"conversation_id\"].unique()\n",
    "train_ids, test_ids = train_test_split(conversation_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = df[df[\"conversation_id\"].isin(train_ids)].reset_index(drop=True)\n",
    "test_df = df[df[\"conversation_id\"].isin(test_ids)].reset_index(drop=True)\n",
    "print(train_df.head())\n",
    "\n",
    "# Convert back to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "\n",
    "# Optional: Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine-tuned-llama2\", use_fast=True)\n",
    "\n",
    "# Load base model â€” must match original base!\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# Load PEFT model (with LoRA weights applied)\n",
    "model = PeftModel.from_pretrained(base_model, \"./fine-tuned-llama2\")\n",
    "\n",
    "# -----------------------\n",
    "# PEFT Configuration (LoRA)\n",
    "# -----------------------\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.5,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# -----------------------\n",
    "# Fine-Tuning\n",
    "# -----------------------\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    report_to=\"none\",\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff61ded-db78-4757-b780-983474ead1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f8f5c-6830-4bdd-9662-4155bda3d510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "torch.cuda.empty_cache()\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_test_dataset = dataset_dict[\"test\"].map(tokenize, batched=True)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = trainer.predict(tokenized_test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Generate detailed classification report (using integer labels)\n",
    "print(\"Detailed Classification Report (Integer Labels):\")\n",
    "print(classification_report(true_labels, preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294eb9d-2f77-47d9-b391-51a8f6b56734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine-tuned-llama2\", use_fast=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",  # or whatever base model you originally used\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Apply LoRA fine-tuned weights\n",
    "model = PeftModel.from_pretrained(base_model, \"./fine-tuned-llama2\")\n",
    "model.eval()\n",
    "\n",
    "def generate_response(prompt, max_new_tokens=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage:\n",
    "prompt = \"Conversation:\\nTutor: Hi, could you please provide a step-by-step solution for the question below? The question is: Tyson decided to make muffaletta sandwiches for the big game.\\xa0\\xa0Each sandwich required 1 pound each of meat and cheese and would serve 4 people.\\xa0\\xa0There would be 20 people in total watching the game.\\xa0\\xa0The meat cost $7.00 per pound and the cheese cost $3.00 per pound.\\xa0\\xa0How much money would he spend on the meat and cheese to make enough sandwiches to serve 20 people?\\xa0\\n\\xa0Student: To serve 20 people, Tyson needs to make 20/4 = 5 sandwiches.\\nEach sandwich requires 1+1 = 2 pounds of meat and cheese.\\nFor 5 sandwiches, he needs a total of 2 x 5 = 10 pounds of meat and cheese.\\nThe cost of 10 pounds of meat is 10 x $7.00 = $70.\\nThe cost of 10 pounds of cheese is 10 x $3.00 = $30.\\nThe total cost of meat and cheese is $70 + $30 = $100.\\n\\xa0100\\xa0\\n\\xa0Tutor: How many pounds of meat are needed for each sandwich?\\xa0\\n\\xa0Student: Each sandwich requires 1 pound of meat and 1 pound of cheese.\\xa0\\n\\xa0Tutor: What is the cost of 1 pound of meat?\\xa0\\n\\xa0Student: The cost of 1 pound of meat is $7.00.\\n\\nResponse:\\nGreat, you've correctly identified the cost of the meat, now let's focus on calculating the total cost of meat for all the sandwiches needed.\\n\\n\\nPrediction: \"\n",
    "print(generate_response(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b6bdb-6e52-4162-8927-99fc42acb709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/mrbench_v3_devset_train_data.csv\").dropna(subset=[\"response\"])\n",
    "df[\"text\"] = \"Conversation:\\n\" + df[\"conversation_history\"] + \"\\n\\nResponse:\\n\" + df[\"response\"] + \"\\n\\n\\nPrediction: \"\n",
    "df[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9421b6c3-1cc3-4584-b29c-1a108836e50c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st-fake-news-kernel",
   "language": "python",
   "name": "st-fake-news-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
